{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyPqgZlkKFdU+24WGRDv/Cot","include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10032722,"sourceType":"datasetVersion","datasetId":6179298}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/BrunoFelalaga/diffusion_model_for_scanned_pdf_denoising/blob/main/ddm_implicit_bb_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"id":"j_2g2lNErNgD","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:50:57.611755Z","iopub.execute_input":"2024-11-27T18:50:57.612026Z","iopub.status.idle":"2024-11-27T18:50:57.619775Z","shell.execute_reply.started":"2024-11-27T18:50:57.611998Z","shell.execute_reply":"2024-11-27T18:50:57.619123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(f\"tf version: {tf.__version__}\\n\\n\")\n\n!pip install tensorflow==2.17.1 --upgrade\n\nprint(f\"\\n\\ntf version: {tf.__version__}\")\n\n\n# import tensorflow as tf\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\n# Set the mixed precision policy\nset_global_policy('mixed_float16')\nprint(f\"Mixed precision policy set to: {tf.keras.mixed_precision.global_policy()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:22:29.615176Z","iopub.execute_input":"2024-11-27T19:22:29.615959Z","iopub.status.idle":"2024-11-27T19:22:38.272201Z","shell.execute_reply.started":"2024-11-27T19:22:29.615927Z","shell.execute_reply":"2024-11-27T19:22:38.271008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# from google.colab import drive\n\n# drive.mount('/content/gdrive')\n\n\n\n# diffusion_nbs = '/content/gdrive/MyDrive/bon_vision/genai_cls/wk_7_GAN_n_diffusion_n_realnvp_models/notebooks-1'\n\n# %cd $diffusion_nbs\n","metadata":{"id":"-VYZ_UfPrWfi","outputId":"d52e387e-6aac-43b1-b937-8e82675eee57","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:34:15.931234Z","iopub.status.idle":"2024-11-27T18:34:15.931700Z","shell.execute_reply.started":"2024-11-27T18:34:15.931482Z","shell.execute_reply":"2024-11-27T18:34:15.931509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/noisy-scan-images/prob_imgs | wc -l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:35:51.456021Z","iopub.execute_input":"2024-11-27T18:35:51.457344Z","iopub.status.idle":"2024-11-27T18:35:52.676702Z","shell.execute_reply.started":"2024-11-27T18:35:51.457278Z","shell.execute_reply":"2024-11-27T18:35:52.675254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport math\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport keras\nfrom keras import layers\nfrom keras import ops","metadata":{"id":"rLrPbZZbrW7r","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:23:00.725949Z","iopub.execute_input":"2024-11-27T19:23:00.726954Z","iopub.status.idle":"2024-11-27T19:23:00.731647Z","shell.execute_reply.started":"2024-11-27T19:23:00.726918Z","shell.execute_reply":"2024-11-27T19:23:00.730738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data\n\ndataset_name = \"problematic_pdf_imgs\" #\"oxford_flowers102\"\ndataset_repetitions = 5\nnum_epochs = 50  # train for at least 50 epochs for good results\nimage_size = 128# from 64\n\n# KID = Kernel Inception Distance, see related section\nkid_image_size = 75\nkid_diffusion_steps = 5\nplot_diffusion_steps = 20\n\n# sampling\nmin_signal_rate = 0.02\nmax_signal_rate = 0.95\n\n\n# architecture\nembedding_dims = 32\nembedding_max_frequency = 1000.0\nwidths = [32, 64, 96, 128]\nblock_depth = 2\n\n\n# optimization\n\nbatch_size = 64\nema = 0.999\nlearning_rate = 1e-4 # 1e-3\nweight_decay = 1e-4\n\n# for loading saved model or training\nLOAD_SAVED = False\n\n# RGB or GRAY\nGRAY = True","metadata":{"id":"utq5MaORrZY_","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:23:02.175050Z","iopub.execute_input":"2024-11-27T19:23:02.175930Z","iopub.status.idle":"2024-11-27T19:23:02.181305Z","shell.execute_reply.started":"2024-11-27T19:23:02.175895Z","shell.execute_reply":"2024-11-27T19:23:02.180417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nfrom glob import glob\n\ndef load_files(image_dir, split_ratio=0.8):\n    # Get all image files from directory\n    image_patterns = ['*.jpg', '*.jpeg', '*.png']  # Define supported image patterns\n    image_files = []\n    for pattern in image_patterns:\n        image_files.extend(glob(os.path.join(image_dir, pattern)))  # Collect all image paths\n\n    np.random.shuffle(image_files)  # Shuffle file order randomly\n    split_idx = int(len(image_files) * split_ratio)  # Calculate split index for train/val\n\n    return image_files[:split_idx], image_files[split_idx:]  # Return split file lists\n\ndef preprocess_image(image_path, image_size, is_gray=False):\n    # Load and decode image\n    image = tf.io.read_file(image_path)  # Read image file from disk\n    image = tf.image.decode_image(image, channels=3, expand_animations=False)  # Decode to tensor with 3 channels\n    if is_gray == True:\n      image = tf.image.rgb_to_grayscale(image)  # Convert RGB to Grayscale (1 channel) XXXXXXX----------------------------------BW\n    image = tf.cast(image, tf.float32)  # Convert to float32 for processing\n\n    # Center crop image\n    height = tf.shape(image)[0]  # Get image height\n    width = tf.shape(image)[1]   # Get image width\n    crop_size = tf.minimum(height, width)  # Determine smaller dimension for cropping\n\n    image = tf.image.crop_to_bounding_box(\n        image,\n        (height - crop_size) // 2,  # Calculate top offset for center crop\n        (width - crop_size) // 2,   # Calculate left offset for center crop\n        crop_size,                  # Set crop height\n        crop_size,                  # Set crop width\n    )\n\n    # Resize and normalize\n    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)  # Resize image with antialiasing\n    return tf.clip_by_value(image / 255.0, 0.0, 1.0)  # Normalize pixel values to [0, 1]\n\ndef create_dataset(file_list, batch_size, image_size, repeat=1, is_gray=False):\n    # Create and configure dataset pipeline\n    return (\n        tf.data.Dataset.from_tensor_slices(file_list)  # Create dataset from file paths\n        .map(lambda x: preprocess_image(x, image_size, is_gray=is_gray),\n             num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing in parallel\n        .cache()  # Cache processed images in memory\n        .repeat(repeat)  # Repeat dataset for multiple epochs\n        .shuffle(10 * batch_size)  # Shuffle with buffer of 10x batch_size\n        .batch(batch_size, drop_remainder=True)  # Create batches, dropping incomplete ones\n        .prefetch(tf.data.AUTOTUNE)  # Prefetch next batch while processing current one\n    )\n\ndef load_custom_dataset(image_dir, batch_size=64, image_size=64, split_ratio=0.8, repeat=1, is_gray=False):\n    # Split files into train and validation sets\n    train_files, val_files = load_files(image_dir, split_ratio)  # Get train/val file lists\n\n    # Create train and validation datasets\n    train_dataset = create_dataset(train_files, batch_size, image_size, repeat, is_gray)  # Prepare training dataset\n    val_dataset = create_dataset(val_files, batch_size, image_size, repeat, is_gray)  # Prepare validation dataset\n\n    return train_dataset, val_dataset\n\n# \"/kaggle/input/noisy-scan-images/prob_imgs\"#\"/","metadata":{"id":"u177yZ5_rbh-","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:23:06.542267Z","iopub.execute_input":"2024-11-27T19:23:06.542621Z","iopub.status.idle":"2024-11-27T19:23:06.554277Z","shell.execute_reply.started":"2024-11-27T19:23:06.542589Z","shell.execute_reply":"2024-11-27T19:23:06.553302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration parameters\nimage_dir = \"/kaggle/input/noisy-scan-images/prob_imgs\"#\"/content/gdrive/MyDrive/bon_vision/genai_cls/wk_7_GAN_n_diffusion_n_realnvp_models/prob_imgs\"  # Image directory path\nbatch_size = 4  # 64 Number of images per batch\nimage_size = 512  # 64 Target image dimensions\nrepeat = 5  # Number of dataset repetitions\n# print(GRAY)\n# Load and prepare datasets\ntrain_dataset, val_dataset = load_custom_dataset( image_dir=image_dir,\n                                                  batch_size=batch_size,\n                                                  image_size=image_size,\n                                                  split_ratio=0.8,  # 80% for training, 20% for validation\n                                                  repeat=repeat, is_gray=GRAY)  # Repeat dataset for training\n\n# for batch in train_dataset.take(1):\n#     print(\"Batch shape:\", batch.shape)  # Should output (batch_size, image_size, image_size, 1)\n","metadata":{"id":"syLmsG1NrdN1","outputId":"9718bdae-e941-464f-d3bb-b3af5aef0ebd","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:23:12.674987Z","iopub.execute_input":"2024-11-27T19:23:12.675308Z","iopub.status.idle":"2024-11-27T19:23:30.156981Z","shell.execute_reply.started":"2024-11-27T19:23:12.675281Z","shell.execute_reply":"2024-11-27T19:23:30.156209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Visualize one image from the training dataset\nfor batch in train_dataset.take(1):  # Take one batch\n    print(\"Batch shape:\", batch.shape)  # Should output (batch_size, image_size, image_size, 1)\n    image = batch[0]  # Extract the first image from the batch\n    break\n\n# Display the image\nif GRAY:\n    plt.imshow(tf.squeeze(image), cmap='gray')  # For grayscale images\nelse:\n    plt.imshow(image.numpy())  # For RGB images\nplt.axis(\"off\")  # Hide axis\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:23:32.990578Z","iopub.execute_input":"2024-11-27T19:23:32.991281Z","iopub.status.idle":"2024-11-27T19:23:48.855531Z","shell.execute_reply.started":"2024-11-27T19:23:32.991246Z","shell.execute_reply":"2024-11-27T19:23:48.854111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# @keras.saving.register_keras_serializable()\nfrom tensorflow.keras.utils import register_keras_serializable\n\n@register_keras_serializable()\nclass KID(keras.metrics.Metric):\n    def __init__(self, name, is_gray=False, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        # KID is estimated per batch and is averaged across batches\n        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n\n        # Determine input channels dynamically\n        input_channels = 1 if is_gray else 3\n\n        # a pretrained InceptionV3 is used without its classification layer\n        # transform the pixel values to the 0-255 range, then use the same\n        # preprocessing as during pretraining\n        self.encoder = keras.Sequential(\n            [\n                keras.Input(shape=(image_size, image_size, 3)),\n                layers.Rescaling(255.0),\n                layers.Resizing(height=kid_image_size, width=kid_image_size),\n                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n                keras.applications.InceptionV3(\n                    include_top=False,\n                    input_shape=(kid_image_size, kid_image_size, input_channels),\n                    weights=None if is_gray else \"imagenet\",\n                ),\n                layers.GlobalAveragePooling2D(),\n            ],\n            name=\"inception_encoder\",\n        )\n\n    # def polynomial_kernel(self, features_1, features_2):\n    #     feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype=\"float32\")\n    #     return (\n    #         features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0\n    #     ) ** 3.0\n\n    def polynomial_kernel(self, features_1, features_2):\n        # Ensure both inputs are of the same dtype\n        dtype = features_1.dtype\n        features_2 = tf.cast(features_2, dtype)\n        \n        feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype)\n        return (\n            features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0\n        ) ** 3.0\n\n\n    def update_state(self, real_images, generated_images, sample_weight=None):\n        real_features = self.encoder(real_images, training=False)\n        generated_features = self.encoder(generated_images, training=False)\n\n        # compute polynomial kernels using the two sets of features\n        kernel_real = self.polynomial_kernel(real_features, real_features)\n        kernel_generated = self.polynomial_kernel( generated_features, generated_features)\n        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n\n        # estimate the squared maximum mean discrepancy using the average kernel values\n        batch_size = real_features.shape[0]\n        batch_size_f = ops.cast(batch_size, dtype=\"float32\")\n        mean_kernel_real = ops.sum(kernel_real * (1.0 - ops.eye(batch_size))) / (\n            batch_size_f * (batch_size_f - 1.0)\n        )\n        mean_kernel_generated = ops.sum(\n            kernel_generated * (1.0 - ops.eye(batch_size))\n        ) / (batch_size_f * (batch_size_f - 1.0))\n        mean_kernel_cross = ops.mean(kernel_cross)\n        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n\n        # update the average KID estimate\n        self.kid_tracker.update_state(kid)\n\n    def result(self):\n        return self.kid_tracker.result()\n\n    def reset_state(self):\n        self.kid_tracker.reset_state()\n","metadata":{"id":"Xev6E3lTre1f","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:37.079397Z","iopub.execute_input":"2024-11-27T19:35:37.080120Z","iopub.status.idle":"2024-11-27T19:35:37.091424Z","shell.execute_reply.started":"2024-11-27T19:35:37.080084Z","shell.execute_reply":"2024-11-27T19:35:37.090527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# print(tf.__version__)\n\n# !pip install tensorflow==2.17.1 --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:39.150028Z","iopub.execute_input":"2024-11-27T19:35:39.150364Z","iopub.status.idle":"2024-11-27T19:35:39.154441Z","shell.execute_reply.started":"2024-11-27T19:35:39.150333Z","shell.execute_reply":"2024-11-27T19:35:39.153586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_network(image_size, widths, block_depth, is_gray=False):\n    # Determine the number of input and output channels\n    input_channels = 1 if is_gray else 3\n    output_channels = 1 if is_gray else 3\n\n    # Define inputs\n    noisy_images = keras.Input(shape=(image_size, image_size, input_channels))\n    noise_variances = keras.Input(shape=(1, 1, 1))\n\n    # Sinusoidal embedding for noise variances\n    # e = layers.Lambda(sinusoidal_embedding, output_shape=(1, 1, 32))(noise_variances)\n    # e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n    e = SinusoidalEmbeddingLayer( embedding_dims=embedding_dims,embedding_max_frequency=embedding_max_frequency)(noise_variances)\n    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n\n\n    # Initial convolution\n    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n    x = layers.Concatenate()([x, e])\n    skips = []\n\n    # Downsampling path\n    for width in widths[:-1]:\n        x = DownBlock(width, block_depth)([x, skips])\n\n    # Bottleneck (middle block)\n    for _ in range(block_depth):\n        x = ResidualBlock(widths[-1])(x)\n\n    # Upsampling path\n    for width in reversed(widths[:-1]):\n        x = UpBlock(width, block_depth)([x, skips])\n\n    # Final convolution to match output channels\n    x = layers.Conv2D(output_channels, kernel_size=1, kernel_initializer=\"zeros\")(x)\n\n    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n","metadata":{"id":"ifYuatkwrgxt","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:40.750020Z","iopub.execute_input":"2024-11-27T19:35:40.750844Z","iopub.status.idle":"2024-11-27T19:35:40.757594Z","shell.execute_reply.started":"2024-11-27T19:35:40.750807Z","shell.execute_reply":"2024-11-27T19:35:40.756703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @keras.saving.register_keras_serializable()\nfrom tensorflow.keras.utils import register_keras_serializable\n\n# @register_keras_serializable()\n# def sinusoidal_embedding(x):\n#     # Define minimum frequency for embeddings\n#     embedding_min_frequency = 1.0\n\n    # Generate logarithmically spaced frequencies\n    # frequencies = ops.exp(ops.linspace( ops.log( embedding_min_frequency),\n    #                                     ops.log(embedding_max_frequency),\n    #                                     embedding_dims // 2,))\n\n    # # Compute angular speeds for sinusoidal functions\n    # angular_speeds = ops.cast(2.0 * math.pi * frequencies, \"float32\")\n\n    # # Concatenate sine and cosine embeddings\n    # embeddings = ops.concatenate([ops.sin(angular_speeds * x),\n    #                               ops.cos(angular_speeds * x)], axis=3)\n    # return embeddings\n\n# from tensorflow.keras.layers import Layer\n\n# class SinusoidalEmbeddingLayer(Layer):\n#     def __init__(self, embedding_dims, embedding_max_frequency, **kwargs):\n#         super().__init__(**kwargs)\n#         self.embedding_dims = embedding_dims\n#         self.embedding_max_frequency = embedding_max_frequency\n\n#     def call(self, inputs):\n#         embedding_min_frequency = 1.0\n#         frequencies = tf.exp(tf.linspace(\n#             tf.math.log(embedding_min_frequency),\n#             tf.math.log(self.embedding_max_frequency),\n#             self.embedding_dims // 2,\n#         ))\n#         angular_speeds = 2.0 * math.pi * frequencies\n#         embeddings = tf.concat([\n#             tf.sin(angular_speeds * inputs),\n#             tf.cos(angular_speeds * inputs)\n#         ], axis=-1)\n#         return embeddings\n\nfrom tensorflow.keras.layers import Layer\n\nclass SinusoidalEmbeddingLayer(Layer):\n    def __init__(self, embedding_dims, embedding_max_frequency, **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_dims = embedding_dims\n        self.embedding_max_frequency = embedding_max_frequency\n\n    def call(self, inputs):\n        # Cast inputs to float32 to ensure compatibility with mixed precision\n        inputs = tf.cast(inputs, dtype=tf.float32)\n\n        embedding_min_frequency = 1.0\n        frequencies = tf.exp(tf.linspace(\n            tf.math.log(embedding_min_frequency),\n            tf.math.log(self.embedding_max_frequency),\n            self.embedding_dims // 2,\n        ))\n        angular_speeds = 2.0 * math.pi * frequencies\n        embeddings = tf.concat([\n            tf.sin(angular_speeds * inputs),\n            tf.cos(angular_speeds * inputs)\n        ], axis=-1)\n        return embeddings\n\n\ndef ResidualBlock(width):\n    # Define a residual block for feature learning\n    def apply(x):\n        input_width = x.shape[3]  # Get input tensor width (number of channels)\n\n        # Match input width to target width with a 1x1 convolution if needed\n        if input_width == width:\n            residual = x  # No adjustment needed\n        else:\n            residual = layers.Conv2D(width, kernel_size=1)(x)  # Adjust channels\n\n        # Apply batch normalization (no scaling or centering)\n        x = layers.BatchNormalization(center=False, scale=False)(x)\n\n        # Apply two 3x3 convolutions with activation for feature extraction\n        x = layers.Conv2D(width, kernel_size=3, padding=\"same\", activation=\"swish\")(x)\n        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n\n        # Add the residual connection\n        x = layers.Add()([x, residual])\n        return x\n    return apply\n\n\ndef DownBlock(width, block_depth):\n    # Define a downsampling block with residual connections\n    def apply(x):\n        x, skips = x  # Separate input tensor and skip connections\n\n        # Apply multiple residual blocks\n        for _ in range(block_depth):\n            x = ResidualBlock(width)(x)  # Extract features\n            skips.append(x)  # Save skip connection\n\n        # Downsample features using average pooling\n        x = layers.AveragePooling2D(pool_size=2)(x)\n        return x\n    return apply\n\n\ndef UpBlock(width, block_depth):\n    # Define an upsampling block with skip connections\n    def apply(x):\n        x, skips = x  # Separate input tensor and skip connections\n\n        # Upsample the tensor to double its spatial size\n        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n\n        # Apply multiple residual blocks with skip connections\n        for _ in range(block_depth):\n            x = layers.Concatenate()([x, skips.pop()])  # Concatenate skip connection\n            x = ResidualBlock(width)(x)  # Extract features\n        return x\n    return apply\n","metadata":{"id":"82y5hPrtrin9","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:42.046155Z","iopub.execute_input":"2024-11-27T19:35:42.046876Z","iopub.status.idle":"2024-11-27T19:35:42.058779Z","shell.execute_reply.started":"2024-11-27T19:35:42.046840Z","shell.execute_reply":"2024-11-27T19:35:42.057859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def diffusion_schedule(self, diffusion_times):\n        # diffusion times -> angles\n        start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")\n        end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")\n        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n        # angles -> signal and noise rates\n        signal_rates = ops.cos(diffusion_angles)\n        noise_rates = ops.sin(diffusion_angles)\n        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n        return noise_rates, signal_rates","metadata":{"id":"hCwefbSjrlO6","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:43.391854Z","iopub.execute_input":"2024-11-27T19:35:43.392551Z","iopub.status.idle":"2024-11-27T19:35:43.397328Z","shell.execute_reply.started":"2024-11-27T19:35:43.392517Z","shell.execute_reply":"2024-11-27T19:35:43.396433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import register_keras_serializable\n\n# @keras.saving.register_keras_serializable()\n@register_keras_serializable()\nclass DiffusionModel(keras.Model):\n    def __init__(self, image_size, widths, block_depth, is_gray=False):\n        super().__init__()\n\n        self.image_size = image_size\n        self.is_gray = is_gray  # Flag to indicate grayscale images\n\n        # Normalization layer to standardize inputs\n        self.normalizer = layers.Normalization()\n\n        # Create the main network with dynamic input channels\n        self.network = get_network(\n            image_size=image_size,\n            widths=widths,\n            block_depth=block_depth,\n            is_gray=self.is_gray  # Pass the grayscale flag\n        )\n\n        # Create an exponential moving average (EMA) of the network\n        self.ema_network = keras.models.clone_model(self.network)\n\n    def compile(self, **kwargs):\n        super().compile(**kwargs)\n\n        # Trackers for noise and image loss\n        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n\n        # Initialize KID metric with grayscale flag\n        self.kid = KID(name=\"kid\", is_gray=self.is_gray)\n\n    @property\n    def metrics(self):\n        # Return the list of metrics for tracking\n        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n\n    def denormalize(self, images):\n        # Convert pixel values back to [0, 1] range from normalized\n        images = self.normalizer.mean + images * self.normalizer.variance ** 0.5\n        return tf.clip_by_value(images, 0.0, 1.0)\n\n    def diffusion_schedule(self, diffusion_times):\n        # Define the start and end angles for diffusion\n        start_angle = tf.cast(tf.acos(max_signal_rate), dtype=\"float32\")\n        end_angle = tf.cast(tf.acos(min_signal_rate), dtype=\"float32\")\n\n        # Compute diffusion angles for given times\n        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n\n        # Calculate signal and noise rates from angles\n        signal_rates = tf.cos(diffusion_angles)\n        noise_rates = tf.sin(diffusion_angles)\n        # Note: sin^2(x) + cos^2(x) = 1\n\n        return noise_rates, signal_rates\n\n    # def denoise(self, noisy_images, noise_rates, signal_rates, training):\n    #     # Select the appropriate network (EMA network during inference)\n    #     network = self.network if training else self.ema_network\n\n    #     # Predict noise component\n    #     pred_noises = network([noisy_images, noise_rates ** 2], training=training)\n    #     # Calculate the predicted clean image\n    #     pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n\n    #     return pred_noises, pred_images\n\n    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n        # Ensure all tensors have the same type as noisy_images\n        noise_rates = tf.cast(noise_rates, noisy_images.dtype)\n        signal_rates = tf.cast(signal_rates, noisy_images.dtype)\n    \n        network = self.network if training else self.ema_network\n    \n        pred_noises = network([noisy_images, noise_rates ** 2], training=training)\n        pred_noises = tf.cast(pred_noises, noisy_images.dtype)  # Match dtype explicitly\n    \n        # Perform operations with consistent tensor types\n        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n    \n        return pred_noises, pred_images\n\n\n\n    def reverse_diffusion(self, initial_noise, diffusion_steps):\n        # Initialize variables for reverse diffusion process\n        num_images = initial_noise.shape[0]\n        step_size = 1.0 / diffusion_steps\n\n        next_noisy_images = initial_noise  # Start with initial noise\n\n        for step in range(diffusion_steps):\n            noisy_images = next_noisy_images\n\n            # Compute current diffusion times\n            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n            # Get noise and signal rates\n            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n\n            # Denoise the images\n            pred_noises, pred_images = self.denoise(\n                noisy_images, noise_rates, signal_rates, training=False\n            )\n\n            # Compute next diffusion times\n            next_diffusion_times = diffusion_times - step_size\n            # Get next noise and signal rates\n            next_noise_rates, next_signal_rates = self.diffusion_schedule(next_diffusion_times)\n\n            # Prepare noisy images for next step\n            next_noisy_images = (\n                next_signal_rates * pred_images + next_noise_rates * pred_noises\n            )\n\n        return pred_images  # Return the final denoised images\n\n    def generate(self, num_images, diffusion_steps):\n        # Determine number of channels based on grayscale flag\n        input_channels = 1 if self.is_gray else 3\n\n        # Generate initial random noise\n        initial_noise = tf.random.normal(\n            shape=(num_images, self.image_size, self.image_size, input_channels)\n        )\n\n        # Perform reverse diffusion to generate images\n        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n        # Denormalize images back to [0, 1] range\n        generated_images = self.denormalize(generated_images)\n        return generated_images\n\n    def train_step(self, images):\n        # Determine number of channels\n        input_channels = 1 if self.is_gray else 3\n\n        # Normalize input images\n        images = self.normalizer(images, training=True)\n        # Generate random noise\n        noises = tf.random.normal(\n            shape=(tf.shape(images)[0], self.image_size, self.image_size, input_channels)\n        )\n\n        # Sample random diffusion times uniformly between 0 and 1\n        diffusion_times = tf.random.uniform(\n            shape=(tf.shape(images)[0], 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        # Compute noise and signal rates\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n        # Create noisy images by mixing images with noise\n        noisy_images = signal_rates * images + noise_rates * noises\n\n        with tf.GradientTape() as tape:\n            # Predict noise and images from noisy inputs\n            pred_noises, pred_images = self.denoise(\n                noisy_images, noise_rates, signal_rates, training=True\n            )\n\n            # Calculate losses\n            noise_loss = self.loss(noises, pred_noises)  # Loss on noise prediction\n            image_loss = self.loss(images, pred_images)  # Loss on image reconstruction\n\n        # Compute gradients and apply optimizer\n        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n\n        # Update loss metrics\n        self.noise_loss_tracker.update_state(noise_loss)\n        self.image_loss_tracker.update_state(image_loss)\n\n        # Update EMA weights\n        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n\n        # Return metrics (excluding KID for efficiency)\n        return {m.name: m.result() for m in self.metrics[:-1]}\n\n    def test_step(self, images):\n        # Determine number of channels\n        input_channels = 1 if self.is_gray else 3\n\n        # Normalize input images\n        images = self.normalizer(images, training=False)\n        # Generate random noise\n        noises = tf.random.normal(\n            shape=(tf.shape(images)[0], self.image_size, self.image_size, input_channels)\n        )\n\n        # Sample random diffusion times\n        diffusion_times = tf.random.uniform(\n            shape=(tf.shape(images)[0], 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        # Compute noise and signal rates\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n        # Create noisy images\n        noisy_images = signal_rates * images + noise_rates * noises\n\n        # Predict noise and images without training\n        pred_noises, pred_images = self.denoise(\n            noisy_images, noise_rates, signal_rates, training=False\n        )\n\n        # Calculate losses\n        noise_loss = self.loss(noises, pred_noises)\n        image_loss = self.loss(images, pred_images)\n\n        # Update loss metrics\n        self.image_loss_tracker.update_state(image_loss)\n        self.noise_loss_tracker.update_state(noise_loss)\n\n        # Denormalize images for KID computation\n        images = self.denormalize(images)\n        # Generate new images for comparison\n        generated_images = self.generate(\n            num_images=tf.shape(images)[0], diffusion_steps=kid_diffusion_steps\n        )\n        # Update KID metric\n        self.kid.update_state(images, generated_images)\n\n        # Return all metrics\n        return {m.name: m.result() for m in self.metrics}\n\n    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n        # Generate images using the model\n        generated_images = self.generate(\n            num_images=num_rows * num_cols,\n            diffusion_steps=plot_diffusion_steps,\n        )\n\n        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n        for index, image in enumerate(generated_images):\n            plt.subplot(num_rows, num_cols, index + 1)\n            if self.is_gray:\n                # Use tf.squeeze to remove the channel dimension\n                image = tf.squeeze(image, axis=-1)\n                # Convert tensor to NumPy array for plotting\n                image = image.numpy()\n                # If grayscale, remove channel dimension and use grayscale colormap\n                plt.imshow(image.squeeze(), cmap='gray')\n            else:\n                plt.imshow(image)\n            plt.axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n        plt.close()\n","metadata":{"id":"8P-vWXairmp0","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:45.903754Z","iopub.execute_input":"2024-11-27T19:35:45.904105Z","iopub.status.idle":"2024-11-27T19:35:45.926968Z","shell.execute_reply.started":"2024-11-27T19:35:45.904074Z","shell.execute_reply":"2024-11-27T19:35:45.926175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ~45mins for A100 on 50 epochs with early stopping\nLOAD_SAVED = False\nif LOAD_SAVED:\n  raise SystemExit(\"Skip model training and load pre-trained model\")\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\n# from tensorflow.keras.callbacks import EarlyStopping\n\n\n# create and compile the model\nmodel = DiffusionModel(image_size, widths, block_depth, is_gray=GRAY)\n\n# for below tensorflow 2.9:\n# pip install tensorflow_addons\n# import tensorflow_addons as tfa\n# optimizer=tfa.optimizers.AdamW\nmodel.compile( optimizer=keras.optimizers.AdamW(learning_rate=learning_rate,\n                                                weight_decay=weight_decay),\n                loss=keras.losses.MeanAbsoluteError()) # pixelwise mean absolute error is used as loss\n\n# Early Stopping bb\n# Define Early Stopping\nearly_stopping_callback = EarlyStopping(  monitor=\"val_kid\",  # Metric to monitor (change if needed)\n                                          patience=20,        # Number of epochs with no improvement before stopping\n                                          mode=\"min\",         # Direction: \"min\" for validation loss\n                                          restore_best_weights=True,  # Restore weights from the best epoch\n                                          verbose=1 )           # Print when stopping occurs\n\n\n# save the best model based on the validation KID metric\n# model_num = len(os.listdir(f'{diffusion_nbs}/trained_models/ddim_implicit_bb_models'))\n# checkpoint_path = f'{diffusion_nbs}/trained_models/ddim_implicit_bb_models/ddim_implicit_bb_v{model_num}.weights.h5'\ncheckpoint_path = \"kaggle/working/ddim_implicit_bb_vX.weights.h5\"\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                      save_weights_only=True,\n                                                      monitor=\"val_kid\",\n                                                      mode=\"min\",\n                                                      save_best_only=True,)\n\n# Update the Normalization layer to use float32\nmodel.normalizer = layers.Normalization(dtype=\"float32\")\n\n# calculate mean and variance of training dataset for normalization\nmodel.normalizer.adapt(train_dataset)\n\n# run training and plot generated images periodically\nmodel.fit(  train_dataset,\n            epochs= 200, # num_epochs = 50, also stops at 50 with early stopping\n            validation_data=val_dataset,\n            callbacks=[ keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n                        checkpoint_callback, early_stopping_callback, ], )","metadata":{"id":"za0qLVXArpgq","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:35:55.976620Z","iopub.execute_input":"2024-11-27T19:35:55.977337Z","iopub.status.idle":"2024-11-27T19:37:21.026979Z","shell.execute_reply.started":"2024-11-27T19:35:55.977299Z","shell.execute_reply":"2024-11-27T19:37:21.025730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# image_dir = \"/kaggle/input/noisy-scan-images/prob_imgs\"#\"/content/gdrive/MyDrive/bon_vision/genai_cls/wk_7_GAN_n_diffusion_n_realnvp_models/prob_imgs\"  # Image directory path\n# batch_size = 64  # Number of images per batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:34:15.926727Z","iopub.status.idle":"2024-11-27T18:34:15.927315Z","shell.execute_reply.started":"2024-11-27T18:34:15.927016Z","shell.execute_reply":"2024-11-27T18:34:15.927051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LOAD_SAVED = True\n# if not LOAD_SAVED:\n#   raise SystemExit(\"Skip model training and load pre-trained model\") #, ignore_errors=True)\n\n# load the best model and generate images\n# checkpoint_path = f'{diffusion_nbs}/trained_models/ddim_implicit_bb_models/ddim_implicit_bb_v3.weights.h5'\ncheckpoint_path = \"kaggle/working/ddim_implicit_bb_vX.weights.h5\"\n\nmodel = DiffusionModel(image_size, widths, block_depth, is_gray=GRAY)\nmodel.normalizer.adapt(train_dataset) # adapt the normalizer before using the pre-trained model\nmodel.load_weights(checkpoint_path)\nmodel.plot_images()","metadata":{"id":"P6kLSQCIrsiM","outputId":"bbbee823-9ad1-4892-e0e9-0cbdfde2ae96","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:48:08.909003Z","iopub.execute_input":"2024-11-27T18:48:08.910211Z","iopub.status.idle":"2024-11-27T18:48:25.639942Z","shell.execute_reply.started":"2024-11-27T18:48:08.910163Z","shell.execute_reply":"2024-11-27T18:48:25.637944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ODfrDwFfstwS","trusted":true},"outputs":[],"execution_count":null}]}